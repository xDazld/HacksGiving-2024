{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMs on Rosie\n",
    "===\n",
    "MAIC - Fall, Week 10<br>\n",
    "```\n",
    "  _____________\n",
    " /0   /     \\  \\\n",
    "/  \\ M A I C/  /\\\n",
    "\\ / *      /  / /\n",
    " \\___\\____/  @ /\n",
    "          \\_/_/\n",
    "```\n",
    "(Run on Rosie)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Methods of running LLMs**\n",
    "\n",
    "Theory is great and all, but how can one actually run an LLM?\n",
    "\n",
    "[Llama.cpp](https://github.com/ggerganov/llama.cpp) is a solution for running LLMs locally!  \n",
    "It could only run Llama initially, but it can now run most open source LLMs.\n",
    "Fun fact: llama.cpp does not depend on any machine learning or tensor libraries (like Tensorflow or Pytorch, each of which are hundereds of megabytes); it was written from scratch in C/C++.\n",
    "\n",
    "Another solution for running LLMs locally: [Hugging Face Transformers](https://huggingface.co/docs/transformers/index)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:#55ff55;font-weight:bold;font-size:1.5rem;\">\n",
    "    GO\n",
    "</span>\n",
    "\n",
    "But how does one use Llama.cpp?\n",
    "\n",
    "To use it on Rosie, do the following:\n",
    "\n",
    "- Connect to MSOE via VPN.\n",
    "- Start a VSCode server on the [Rosie dashboard](https://dh-ood.hpc.msoe.edu/pun/sys/dashboard).\n",
    "- Make sure you have the python extension installed.\n",
    "  - If you're unfamiliar, take a look at this [extension installation guide](https://code.visualstudio.com/docs/editor/extension-marketplace)\n",
    "  - Search for and install the extension named `Python`. It should be the one with around 3 or 4 million downloads.\n",
    "- Open the command palette (F1 or Ctrl+Shift+P).\n",
    "- Search for and select `Python: Select Interpreter`\n",
    "- Choose the first option: `Enter interpreter path...`\n",
    "- Choose the first option again: `Find...`\n",
    "- Copy & paste `/data/ai_club/team_3_2024-25/team3-env-py312-glibc/bin/python`. Press enter.\n",
    "- Ignore all errors that may pop up (they shouldn't matter)\n",
    "- In the upper-right corner, in this notebook toolbar, select a kernel.\n",
    "- Choose `select another kernel`. (This may be skipped if you haven't used VSCode through the Rosie dashboard.)\n",
    "- Choose `Python Environments`\n",
    "- Choose `team3-env-py312-glibc`\n",
    "- Now you can run code. In the future you should only have to start from selecting a kernel.\n",
    "- Import `lamma_cpp` to see if things are working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#ff5555;font-weight:bold;font-size:1.5rem;\">\n",
    "    STOP\n",
    "</span>\n",
    "\n",
    "... or keep going if you want to work ahead.\n",
    "\n",
    "---\n",
    "\n",
    "Now that Llama.cpp is working, the next step is to load the weights for an LLM model.\n",
    "\n",
    "Llama.cpp supports models stored in the [gguf](https://huggingface.co/docs/hub/en/gguf) weight format.\n",
    "\n",
    "Hugging Face has plenty of [models](https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF) in the gguf format to download, but Rosie already has a few of these models installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls -lah /data/ai_club/llms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use any of these models (although some need more than 1 GPU). For now, let's use `llama-2-7b-chat.Q5_K_M.gguf`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<span style=\"color:#55ff55;font-weight:bold;font-size:1.5rem;\">\n",
    "    GO\n",
    "</span>\n",
    "\n",
    "Let's load our model of choice and use it to complete some text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Llama(\n",
    "    model_path='/data/ai_club/llms/llama-2-7b-chat.Q5_K_M.gguf',\n",
    "    n_gpu_layers=-1, # Put all layers in GPU memory\n",
    "    verbose=False, # A lot of extra info is printed if this isnt set\n",
    "    n_ctx=1000, # Maximum number of input tokens\n",
    "    logits_all=True # Allow logit (token probability) viewing and manipulation for later.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Hello, my name is'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm(prompt) # To use a model, just call it like a function. If this cell takes longer than 1s, then something is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view response\n",
    "response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the response is more than just raw text.\n",
    "\n",
    "In addition to returned text, the Llama.cpp interface also returns additional information. This information is returned as a Python `dict` (dictionary).\n",
    "\n",
    "Dictionaries are like lists. They store a bunch of arbitrary data at various indexes. However, they also allow indexes to be non-integers.\n",
    "\n",
    "```python\n",
    "my_list = [1,2,3]\n",
    "print(my_list[1]) # => 2\n",
    "print(my_list['hello']) # => ERROR, cant index a list with a string\n",
    "\n",
    "my_dict = {\n",
    "    0: 'entry 1',\n",
    "    1: [0, 1, 2],\n",
    "    1.5: 'text',\n",
    "    'a': 'b'\n",
    "    'nested_dict': {1: 2, 'c': 'd'}\n",
    "}\n",
    "print(my_dict[0]) # => 'entry 1'\n",
    "print(my_dict['a']) # => 'b'\n",
    "print(my_dict['goodbye']) # => ERROR, 'goodbye' isn't in my_dict\n",
    "```\n",
    "\n",
    "In the case of our response from Llama.cpp, you can extract the response like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response['choices'][0]['text']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a full text-completion example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Hello, my name is' # Experiment with this\n",
    "\n",
    "response = llm(prompt)\n",
    "\n",
    "print(prompt + response['choices'][0]['text'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#ff5555;font-weight:bold;font-size:1.5rem;\">\n",
    "    STOP\n",
    "</span>\n",
    "\n",
    "... or keep going if you want to work ahead.\n",
    "\n",
    "---\n",
    "\n",
    "There are many ways to control the generation of text.\n",
    "\n",
    "- `max_tokens`: increase this for longer maximum outputs.\n",
    "- `temperature`: this is a common control for LLMs. 0 means the output should be the same every time. Larger values make the output more random.\n",
    "  - More specifically, temperature changes *how* tokens are selected. When completing text, the model predicts the probability for every possible next *token* (where a token is a part of a word or a whole word). When the temperature is 0, the most likely token is always selected. When the temperature is 1, the tokens are selected according to how likely the model thinks each one is. This will become more apparent when looking at the output token probabilities below. \n",
    "- `frequency_penalty`: prevents tokens from being repeated in the output. Repeated tokens are a relatively frequent problem with LLM text generation.\n",
    "- `logprobs`: lets the output include the top-N most likely tokens considered for each position. If the temperature is zero, then the top token will always be chosen.\n",
    "\n",
    "---\n",
    "\n",
    "<span style=\"color:#55ff55;font-weight:bold;font-size:1.5rem;\">\n",
    "    GO\n",
    "</span>\n",
    "\n",
    "Let's mess with these text generation parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Hello, my name is' # Experiment with this\n",
    "\n",
    "response = llm(\n",
    "    prompt,\n",
    "    # Experiment with these parameters\n",
    "    max_tokens=10,\n",
    "    temperature=0,\n",
    "    frequency_penalty=0,\n",
    "    logprobs=3\n",
    ")\n",
    "\n",
    "print(prompt + response['choices'][0]['text']) # Print prompt & output\n",
    "print('\\n===\\n') # to separate cell outputs\n",
    "\n",
    "# Print the token logprobs\n",
    "print('\"selected token\"\\n\\t\"potential token\": logprob\\n\\t...')\n",
    "for logprobs, tok in zip(response['choices'][0]['logprobs']['top_logprobs'], response['choices'][0]['logprobs']['tokens']):\n",
    "    print(f'\"{tok}\"',)\n",
    "    for k,v in logprobs.items():\n",
    "        print(f'\\t\"{k}\": {v:.2f}')\n",
    "    print() # newline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#ff5555;font-weight:bold;font-size:1.5rem;\">\n",
    "    STOP\n",
    "</span>\n",
    "\n",
    "... or keep going if you want to work ahead.\n",
    "\n",
    "---\n",
    "\n",
    "If you take a look at the token probabilities above, you'll notice a relationship between temperature and the selected token.\n",
    "\n",
    "When the temperature is 0, the most likely token is _always_ selected. As the temperature increases, the most likely tokens are still more likely to be selected, but it won't always be the top one.\n",
    "\n",
    "This is cool and all, but how can we do chat-bot interaction like ChatGPT?\n",
    "\n",
    "On top of text-completion abilities, Llama.cpp can also do chat-like inputs with the `create_chat_completion` method.\n",
    "\n",
    "The input to this method is the chat history, which is a list of dictionaries. Each dictionary is a message which stores a `role` (\"who\" said the message), and some `content` (the message itself).\n",
    "\n",
    "There are only a few possible message roles. You can't specify your own.\n",
    "- `system` tells the model what to do (the \"boss\" of the model).\n",
    "- `user` can say anything to the model, and this is what the model is actually responding to.\n",
    "- `assistant` the model itself. You usually dont specify this manually; the model generates these.\n",
    "\n",
    "---\n",
    "\n",
    "<span style=\"color:#55ff55;font-weight:bold;font-size:1.5rem;\">\n",
    "    GO\n",
    "</span>\n",
    "\n",
    "Below is a simple input history to prompt the LLM. The system message is giving the LLM a personality, and a user message is making a request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a history input.\n",
    "\n",
    "history = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'You preface every message with a tangent talking about MAIC (the MSOE AI Club) in every response.' # You can change this.\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Hello. Print something in Python code.'\n",
    "    }\n",
    "]\n",
    "response = llm.create_chat_completion(history) # as mentioned, we use the `create_chat_completion` method with the history\n",
    "\n",
    "# Like before, the result is a complex data structure.\n",
    "# When doing chat completion, the response is in `message` instead of `text`. `message` is a dictionary with a role and generated content.\n",
    "response = response['choices'][0]['message']\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the message text:\n",
    "print(response['content'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#ff5555;font-weight:bold;font-size:1.5rem;\">\n",
    "    STOP\n",
    "</span>\n",
    "\n",
    "... or keep going if you want to work ahead.\n",
    "\n",
    "---\n",
    "\n",
    "**... But how can we continue the conversation?**\n",
    "\n",
    "Transformers. That's how.\n",
    "\n",
    "This chat-completion LLM is still a transformer. The key difference is that it has been \"finetuned\" to differentiate between different roles of a conversation in its input.\n",
    "\n",
    "The text-completion logic is still the same.\n",
    "\n",
    "---\n",
    "\n",
    "<span style=\"color:#55ff55;font-weight:bold;font-size:1.5rem;\">\n",
    "    GO\n",
    "</span>\n",
    "\n",
    "Below is a simple input history to prompt the LLM.\n",
    "\n",
    "The system message is giving the LLM a personality, and a user message is making a request.\n",
    "\n",
    "The remainder of this section will build up how we can go from individual text continuations to full-on chats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serves as the initial history\n",
    "history = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'You are an unhelpful assistant.'\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Write a recursive factorial oneliner in python.'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run everything through the history\n",
    "response = llm.create_chat_completion(history)['choices'][0]['message'] # getting the message itself as we were before\n",
    "print(response) # ... and print the response\n",
    "\n",
    "# Repeat..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the response to the history\n",
    "history.append(response)\n",
    "# Add the next user prompt to the history\n",
    "history.append({\n",
    "    'role': 'user',\n",
    "    'content': 'Now do it in Java.'\n",
    "})\n",
    "\n",
    "# show history so far\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... and then you would call the model again\n",
    "\n",
    "response = llm.create_chat_completion(history)['choices'][0]['message']\n",
    "print(response)\n",
    "\n",
    "# ... put it back in, and repeat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since transformers don't have any memory of the conversation, they have to see everything that the user said, AND everything that **it** said."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can ee that every part of the conversation is in the history:\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a function for this process.\n",
    "def continue_conversation(user_prompt):\n",
    "    # add user prompt to history\n",
    "    history.append(\n",
    "        {\n",
    "            'role':'user',\n",
    "            'content':user_prompt\n",
    "        }\n",
    "    )\n",
    "    # run the model on the entire history\n",
    "    response = llm.create_chat_completion(history)['choices'][0]['message']\n",
    "    # add the model output to the history\n",
    "    history.append(response) # `response` already includes the role\n",
    "\n",
    "    # also return the LLM's latest response text\n",
    "    return response['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(continue_conversation('Write a recursive factorial function in Lua'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(continue_conversation('Write quicksort in C++'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(continue_conversation('What was the first thing I asked you?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the history, formatted nicely this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for msg in history:\n",
    "    print(msg['role'], '--', msg['content'], '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#ff5555;font-weight:bold;font-size:1.5rem;\">\n",
    "    STOP\n",
    "</span>\n",
    "\n",
    "... or keep going if you want to work ahead.\n",
    "\n",
    "---\n",
    "\n",
    "A quick tangent: if the transformer only operates on tokens, then how can messages have \"roles?\"\n",
    "\n",
    "The answer is simple: the list of dictionaries is turning into a list of tokens before being put into the transformers. There are special tokens to indicate the role, so the model is ultimately seeing something like this:\n",
    "```\n",
    "<SYSTEM START TOKEN>You are You are an unhelpful assistant.<SYSTEM END TOKEN><USER START TOKEN>Write a recursive factorial function in Python<USER END TOKEN>\n",
    "```\n",
    "\n",
    "Now let's make a better chat bot!\n",
    "\n",
    "To make the text appear as it's generating, we can use the `stream` parameter of the llm. It changes the result to a python python object which you iterate over to get the tokens.\n",
    "\n",
    "---\n",
    "\n",
    "<span style=\"color:#55ff55;font-weight:bold;font-size:1.5rem;\">\n",
    "    GO\n",
    "</span>\n",
    "\n",
    "Below is the code needed to use the `stream` functionality of Llama.cpp.\n",
    "\n",
    "You will enter chat messages in a VSCode prompt that appears at the middle-top of your screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output # for clearing the output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': 'Talk like an internet chatroom user. Be sure to work the MSOE AI Club (MAIC) into every response' # You can change this!\n",
    "    }\n",
    "]\n",
    "\n",
    "def pretty_print_history(currently_generating):\n",
    "    hist = ''\n",
    "    for msg in history+[currently_generating]:\n",
    "        hist += msg['role'] + ' -- ' + msg['content'] + '\\n'\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_prompt = input()\n",
    "    if user_prompt == '': break\n",
    "    history.append({'role':'user', 'content':user_prompt}) # add user input to history\n",
    "    resp_msg = {'role': '', 'content': ''} # store a dictionary for the generated tokens before adding itself to the history\n",
    "    resp_stream = llm.create_chat_completion(history, stream=True) # generate the token stream\n",
    "    for tok in resp_stream:\n",
    "        delta = tok['choices'][0]['delta'] # the model returns \"deltas\" when streaming tokens. Deltas tell you how to change the response dictionary (resp_msg in this case)\n",
    "        if len(delta) == 0: break # empty delta means it's done\n",
    "        delta_k, delta_v = list(delta.items())[0]\n",
    "        resp_msg[delta_k] += delta_v\n",
    "        clear_output(wait=True)\n",
    "        print(pretty_print_history(resp_msg))\n",
    "        sleep(.1) # This delay makes the output smoother, but you can comment it out\n",
    "    history.append(resp_msg) # Add the full response to the history\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#ff5555;font-weight:bold;font-size:1.5rem;\">\n",
    "    STOP\n",
    "</span>\n",
    "\n",
    "---\n",
    "\n",
    "... What if we make the conversation too long?\n",
    "\n",
    "Transformers have to see the *entire* history to continue it, so there must be a limit on the history length. This limit is often referred to as the the *context length*. This was the `n_ctx` parameter supplied to the initial LLM creation function.\n",
    "\n",
    "---\n",
    "\n",
    "<span style=\"color:#55ff55;font-weight:bold;font-size:1.5rem;\">\n",
    "    GO\n",
    "</span>\n",
    "\n",
    "We will end this workshop with a mini contest. Use the code above to generate the best LLM response.\n",
    "\n",
    "Once we say so, screenshot or copy+paste your responses into the MAIC teams. Submissions will be voted on, and the winner will get a prize ðŸ˜±"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
